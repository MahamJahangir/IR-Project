{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "from lxml import objectify\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " #preprocessing of xml files\n",
    "list=[]\n",
    "list = os.listdir(\"dataset\")\n",
    "for i in range(len(list)):\n",
    "    \n",
    "   \n",
    "    inFile = open(\"dataset/\"+ list[i], 'r')\n",
    "    data = inFile.read()\n",
    "    inFile.close()\n",
    "    data = re.sub(\"\\n\", \"\", data)\n",
    "    data = re.sub(\":\", \"\", data)\n",
    "    data = re.sub(\";\", \"\", data)\n",
    "    data = re.sub(\"-\", \"\", data)\n",
    "    data = re.sub(\"\\ '\", \"\", data)\n",
    "    data = re.sub(\"'\", \"\", data)\n",
    "    data = re.sub(\",\", \"\", data)\n",
    "    data = re.sub(\";\", \"\", data)\n",
    "    data = re.sub(\"|\", \"\", data)\n",
    "    data = re.sub(\"\\t\", \"\", data)\n",
    "    data = re.sub(\"&\", \"\", data)\n",
    "    data = re.sub(\"&eacute;\", \"e\", data)\n",
    "    data = re.sub(\"&aacute;\", \"a\", data)\n",
    "    data = re.sub(\"&yacute;\", \"y\", data)\n",
    "    data = re.sub(\"&nbsp;\", \" \", data)\n",
    "    data = re.sub(\"&tm;\", \"(TM)\", data)\n",
    "    data = re.sub(\"&reg;\", \"(R)\", data)\n",
    "    data = re.sub(\"&agrave;\", \"a\", data)\n",
    "    data = re.sub(\"&egrave;\", \"e\", data)\n",
    "    data = re.sub(\"&igrave\", \"i\", data)\n",
    "    data = re.sub(\"&ecirc;\", \"e\", data)\n",
    "    data = re.sub(\"&ocirc;\", \"o\", data)\n",
    "    data = re.sub(\"&icirc;\", \"i\", data)\n",
    "    data = re.sub(\"&ccedil;\", \"c\", data)\n",
    "    data = re.sub(\"&amp;\", \"and\", data)\n",
    "    data = re.sub(\"&auml;\", \"a\", data)\n",
    "    data = re.sub(\"&szlig;\", \"ss\", data)\n",
    "    data = re.sub(\"&aelig;\", \"e\", data)\n",
    "    data = re.sub(\"&iuml;\", \"i\", data)\n",
    "    data = re.sub(\"&euml;\", \"e\", data)\n",
    "    data = re.sub(\"&ouml;\", \"o\", data)\n",
    "    data = re.sub(\"&uuml;\", \"u\", data)\n",
    "    data = re.sub(\"&acirc;\", \"a\", data)\n",
    "    data = re.sub(\"&oslash;\", \"o\", data)\n",
    "    data = re.sub(\"&ntilde;\", \"n\", data)\n",
    "    data = re.sub(\"&Eacute;\", \"E\", data)\n",
    "    data = re.sub(\"&Aring;\", \"A\", data)\n",
    "    data = re.sub(\"&Ouml;\", \"O\", data)\n",
    "    data = re.sub(\"Å½\", \"e\", data)\n",
    "      # fix \"id=xxx\" pattern, causes XML parsing to fail\n",
    "    data = re.sub(\"\\\"id=\", \"id=\\\"\", data)\n",
    "    outFile = open(\"dataset/\"+ list[i], 'w')\n",
    "    outFile.write(data)\n",
    "    outFile.close()\n",
    "\n",
    "#tree = etree.parse(\"C:/Users/hp/Downloads/06_1.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119,)\n",
      "(119,)\n"
     ]
    }
   ],
   "source": [
    "#data Parsing\n",
    "list=[]\n",
    "list = os.listdir(\"dataset\")\n",
    "\n",
    "features=[]\n",
    "lable=[]\n",
    "\n",
    "for i in range(len(list)):\n",
    "    \n",
    "    \n",
    "    xml=objectify.parse(open(\"dataset/\"+ list[i])) \n",
    "    root=xml.getroot()\n",
    "   \n",
    "    length=len(root.getchildren()[2].getchildren())\n",
    "    \n",
    "    for j in range(length): \n",
    "        cls = root.getchildren()[2].getchildren()[j].findall('class')\n",
    "        sentence=root.getchildren()[2].getchildren()[j].findall('text')\n",
    "            \n",
    "        l_f=np.append(cls,sentence)\n",
    "        features=np.append(features,sentence)\n",
    "        lable=np.append(lable,cls) \n",
    "        \n",
    "print(features.shape)\n",
    "print(lable.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['86 What is immediately apparent in the above is that it attempts neither to locate the alleged duty to Scott Rush in the context of an ongoing AFP investigation nor to justify the imposition of the duty in such a context. Rather it addresses the question whether the circumstances of Lee Rushs providing information to the AFP founded a reasonable expectation in Lee Rush that the AFP would not use and an implied undertaking (though to whom is not stated) not to use that information in a particular way. While that inquiry may be a step in the fashioning of an action for breach of confidence by Lee Rush or a possible duty of care to Lee Rush (if hypothetically the unauthorised disclosure of the information would expose Mr Rush to a reasonably foreseeable risk cf Swinney v Chief Constable of Northumbria Police [1997] QB 464) it does not lead necessarily or obviously to potential relief for Scott Rush in a negligence action by him. At best it hints at Lee Rushs having put the AFP in a position of power over Scott Rush a position which carried with it an obligation not to exercise that power adversely to him. What I should emphasise as well is that whatever claims if any Lee Rush might be able to mount against an officer or officers of the AFP in consequence of their dealings and I do not suggest that the present circumstances suggest he might have any such claims he is not a party to this application.']\n",
      "['cited']\n"
     ]
    }
   ],
   "source": [
    "#spliting dataset as 80% training and 20% testing\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, lable, \n",
    "                                            test_size=0.20, random_state=10)\n",
    "\n",
    "print(features_test[0:1])\n",
    "print(target_test[0:1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "no such child: lower",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-39b18016c239>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#vectorizing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_train_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msrc\\lxml\\objectify.pyx\u001b[0m in \u001b[0;36mlxml.objectify.ObjectifiedElement.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc\\lxml\\objectify.pyx\u001b[0m in \u001b[0;36mlxml.objectify._lookupChildOrRaise\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: no such child: lower"
     ]
    }
   ],
   "source": [
    "#vectorizing\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(features_train[1:2])\n",
    "X_train_counts.shape\n",
    "print(X_train_counts)\n",
    "count_vect = CountVectorizer()\n",
    "X_test_counts = count_vect.fit_transform(features_test)\n",
    "X_test_counts.shape\n",
    "print(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
